{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Scrape Medical Spa Owner Emails using URLLib and Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Open our CSV file and write the header row\n",
    "\n",
    "2) Loop through a list of zip codes representing the geographical center of each state in the U.S.\n",
    "\n",
    "3) Use URLLib to fetch a Search Results Page that lists out \"provider cards\" for every SculpSure provider within a 500-mile radius of each zip code and use Beautiful Soup to parse the html code.\n",
    "\n",
    "4) Open the web page associated with each \"provider card\" and use Beatiful Soup to generate parsed html code, from which we can extract the website URL and all email addresses for the provider.\n",
    "\n",
    "5) Close the CSV file\n",
    "\n",
    "DONE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ZIP: 99777\n",
      "\n",
      "Revive Aesthetics\n",
      "reviveak@gmail.com\n",
      "\n",
      "Nick Sarrimanolis| MD LLC\n",
      "dr.sarrimanolis@icloud.com\n",
      "\n",
      "Integrated Wellness & Center for Birth| LLC\n",
      "shantelhoversten@akbirth.com\n",
      "\n",
      "Alaska Center for Dermatology\n",
      "sculpsure@dermalaska.com\n",
      "\n",
      "Fortson Dermatology and Skin Care Center\n",
      "\n",
      "Borealis Laser\n",
      "borealislaser@gmail.com\n",
      "info@borealislaser.com\n",
      "\n",
      "All Seasons Family Health Care\n",
      "allseasonsfamilyhealthcare@gmail.com\n",
      "office@allseasonsfhc.com\n",
      "\n",
      "Dale Joseph Trombley II| MD\n",
      "info@alaskaprivatepractice.com\n",
      "ConciergeDoctor@quixnet.net\n",
      "\n",
      "SCRAPING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "# Open csv file and write headers\n",
    "filename = \"sculpsure_providers5.csv\"\n",
    "f = open(filename, \"w\", encoding='utf8', errors='ignore')\n",
    "headers = \"name,url,email\\n\"\n",
    "f.write(headers)\n",
    "\n",
    "# Initialize list of providers\n",
    "provider_names = []\n",
    "\n",
    "#zip_list = [1]\n",
    "zip_list = [99777]\n",
    "#zip_list = [1604,2893,3217,4443,5669,6037,8618,13501,16823,19901,21035,23921,26601,27330,29229,31204,34609,35045,37130,39051,40422,43081,46123,49601,50010,54449,56401,57501,58463,59457,62563,65109,67530,68822,71351,72204,73160,76825,80918,82520,83226,84642,86322,87063,89310,93643,96763,97754,98801,99777]\n",
    "\n",
    "for zip_int in zip_list:\n",
    "    zip_str = str(zip_int).rjust(5,'0')\n",
    "    validZip = True\n",
    "\n",
    "    # Process zip code\n",
    "    print(\"Processing ZIP: \" + zip_str)\n",
    "    print()\n",
    "\n",
    "    my_url = 'https://www.sculpsure.com/results/?campaign-code=default&session-id=default&country=us&treatment=33&zipcode=' + zip_str + '&proximity=500'\n",
    "\n",
    "    page = urllib.request.Request(my_url,headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}) \n",
    "    infile = urllib.request.urlopen(page).read()\n",
    "    page_html = infile.decode('ISO-8859-1')\n",
    "\n",
    "    # Get \"page soup\" from Beautiful Soup for the search results page\n",
    "    page_soup = soup(page_html, \"html.parser\")\n",
    "\n",
    "    # If we get a \"notification\", it means no providers for this geographical area\n",
    "    notifications = page_soup.findAll(\"div\",{\"class\":\"notification__content\"})\n",
    "    \n",
    "    if len(notifications) > 0:\n",
    "        print(\"No providers found for zip code: \" + zip_str)\n",
    "    else:\n",
    "        # Get all \"provider cards\" for each SculpSure provider for this geographical area\n",
    "        provider_cards = page_soup.findAll(\"div\",{\"class\":\"provider-card\"})\n",
    "\n",
    "        # Extract data from url associated with each provider card\n",
    "        for provider_card in provider_cards:\n",
    "            provider_name = provider_card.h3.a.text.strip().replace(\",\",\"|\")\n",
    "\n",
    "            if provider_name not in provider_names:\n",
    "                # Add provider to list of providers\n",
    "                provider_names.append(provider_name)\n",
    "                print(provider_name)\n",
    "\n",
    "                # Get provider page html and turn it into parsed soup\n",
    "                provider_url = provider_card.h3.a['href']\n",
    "                provider_page = urllib.request.Request(provider_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                provider_infile = urllib.request.urlopen(provider_page).read()\n",
    "                provider_page_html = provider_infile.decode('ISO-8859-1')\n",
    "                provider_page_soup = soup(provider_page_html, \"html.parser\")\n",
    "\n",
    "                # Get the provider's Website URL\n",
    "                provider_website_urls = provider_page_soup.findAll(\"ul\",{\"class\":\"provider-contact__details\"})\n",
    "\n",
    "                if len(provider_website_urls) > 0:\n",
    "                    provider_website_url = provider_website_urls[0].a['href']                        \n",
    "\n",
    "                    # Get all email addresses for the provider\n",
    "                    provider_form = provider_page_soup.findAll(\"div\",{\"class\":\"provider-form provider-form--left-aligned\"})[0]\n",
    "                    email_script = provider_form.findAll(\"script\")[1].string\n",
    "                    email_start = email_script.find('providersemail') + 18\n",
    "                    email_end = email_script.find('\"',email_start,len(email_script))\n",
    "                    provider_emails = email_script[email_start:email_end].replace(\",\",\"|\")\n",
    "\n",
    "                    # Split up list of emails\n",
    "                    provider_emails_split = provider_emails.split(\";\")\n",
    "\n",
    "                    # Write provider name, website URL and emails to csv file\n",
    "                    f.write(provider_name + \",\" + provider_website_url);\n",
    "                    if len(provider_emails_split) > 0:\n",
    "                        for provider_email in provider_emails_split:\n",
    "                            if provider_email.find('@hologic.com') == -1:\n",
    "                                if provider_email.find('@cynosure.com') == -1:\n",
    "                                    f.write(\",\" + provider_email)\n",
    "                                    print(provider_email)\n",
    "                        print(\"\")\n",
    "                    f.write(\"\\n\")\n",
    "                else:\n",
    "                    f.write(provider_name + \"\\n\")\n",
    "                    print(\"\")\n",
    "                    \n",
    "print(\"SCRAPING COMPLETE!\")\n",
    "\n",
    "# Close csv file\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
